# -*- coding: utf-8 -*-
"""Iris -Sparks Foundation

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tywFl27bWqUhRc_PZt6NszJ_2l-AnMKm

### Task - 2: K- Means Clustering

This notebook will walk through some of the basics of K-Means Clustering.

### Done By: P Sirisha

# Importing Packages
"""

import pandas as pd
from sklearn import datasets
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

"""**Loading the Dataset**"""

data = datasets.load_iris()

"""**Converting to DataFrame**"""

df = pd.DataFrame(data.data, columns = data.feature_names)
df.head()

x = df

"""#Finding the optimum number of clusters for k-means classification

"""

wcss = []
for i in range(1,20):
  model=KMeans(n_clusters = i, init = 'k-means++', random_state = 30)
  model.fit(x)
  wcss.append(model.inertia_)

"""*  If the value of K is huge, then the no. of points within a cluster will be less and hence the inertia will be less
*  Therefore we use elbow method which allows us to pick the optimum no. of clusters for classification.

# Plotting Elbow Technique
"""

plt.plot(range(1,20), wcss)
plt.title("Elbow Technique")
plt.xlabel("No. Of clusters")
plt.ylabel("WCSS")
plt.show()

"""· Using Elbow method and Inertia which is the sum of squared distances of the samples to their closest cluster centre, we founded Optimal no. of clusters required to segment the observations

# Applying kmeans to the dataset
"""

kmeans = KMeans(n_clusters = 3, init = "k-means++", random_state = 30)
y_predict = kmeans.fit_predict(x)

y_predict

kmeans.cluster_centers_

df['cluster'] = y_predict
df

df1 = df[df['cluster'] == 0]
df2 = df[df['cluster'] == 1]
df3 = df[df['cluster'] == 2]

"""**Visualising the clusters - On the first two columns**"""

plt.scatter(df1['sepal length (cm)'], df1['sepal width (cm)'], s=50,c='red', label = 'Iris-setosa')
plt.scatter(df2['sepal length (cm)'], df2['sepal width (cm)'], s=50,c='purple', label = 'Iris-versicolour' )
plt.scatter(df3['sepal length (cm)'], df3['sepal width (cm)'], s=50,c='blue', label = 'Iris-virginica')
plt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],s=200,c='black',marker='+', label = 'centroids')

"""· We can get an absolute segmentation when we put higher K values but if the points with in each cluster are very less then the variation on the real data will be high leading it into over simplifying the data

· So, with K=3 we have obtained an optimal distortion/inertia with which we can segment the data into 3 different clusters with minimal error in segmentation
"""